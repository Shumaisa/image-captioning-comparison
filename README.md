# image-captioning-comparison
Comparison of BLIP, ViT-GPT2, and Microsoft GIT models for image captioning
# Image Captioning Model Comparison

## ğŸ§  Overview

This project compares three state-of-the-art image captioning models:
- **BLIP** (Salesforce)
- **ViT-GPT2** (Hugging Face)
- **Microsoft GIT**

Users can upload an image and the notebook will:
âœ” Generate captions using all three models  
âœ” Measure inference speed  
âœ” Calculate BLEU & ROUGE evaluation metrics  
âœ” Visualize results with bar chart and scatter plots  

---

## ğŸ“Œ Features

- Upload an image in Google Colab
- Compare model performance (speed + quality)
- Plot comparison charts
- Use real benchmarks

---

## ğŸ› ï¸ Libraries Used

This notebook uses the following Python libraries:

- `torch` (PyTorch)
- `transformers` (Hugging Face)
- `Pillow` (Image processing)
- `matplotlib` (Visualizations)
- `nltk` (BLEU score)
- `rouge` (ROUGE score)

---

## ğŸ§¾ Models Used

1. **BLIP** â€“ Salesforce image captioning model  
   (e.g., *Salesforce/blip-image-captioning-base*)  
2. **ViT-GPT2** â€“ Vision Transformer + GPT-2  
   (*nlpconnect/vit-gpt2-image-captioning*)  
3. **Microsoft GIT** â€“ GIT captioning model  
   (*microsoft/git-base-coco*)

---

## ğŸ› ï¸ Tools & Technologies

- **Python** â€“ Primary programming language  
- **Google Colab** â€“ Execution environment
- **GitHub** â€“ Version control & hosting
- **Markdown** â€“ Documentation format
- **Image Processing** â€“ PIL library

---

## ğŸ§ª How to Run

1. Open `main.ipynb` in Google Colab  
2. Install required libraries:  
!pip install -r requirements.txt
3. Upload an image
4. Run all cells

---

## ğŸ“ Folder Structure

image-captioning-comparison/
â”‚
â”œâ”€â”€ main.ipynb
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ sample_images/ â† test images
â”‚ â””â”€â”€ test_image.jpg
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE

---

## ğŸ“Œ License

This project is licensed under the **MIT License**.




